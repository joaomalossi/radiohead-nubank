accessToken<-'64525214-ksyXxMoPb3HwVGqY1TvQAb61QOa8avz3GKpNUEKV6'
accessTokenSecret<-'Wgz25TPADvinQilQqO0UWF8aOtKCq1trlJyYQ2N1zwcBz'
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
## Buscar tweets
tema <- "masterchef"
quantidade.de.tweets <- 1000
lingua <- "pt"
tweets <- searchTwitter(tema, n=quantidade.de.tweets, lang=lingua)
## Converter tweets para um dataframe
tweet.df <- twlistToDF(tweets)
View(tweet.df)
## Data-cleaning e criação de uma wordcloud
library(tm)
library(wordcloud)
library(RColorBrewer)
## Recuperar os textos de cada tweet
textos = sapply(tweets, function(x) x$getText())
## Retornar os 10 primeiros tweets para visualizar
textos[1:10]
## Limpar os textos
textos_tratados = textos
# remove entidades de retweets
textos_tratados = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", textos_tratados)
# remove usuários do Twitter
textos_tratados = gsub("@\\w+", "", textos_tratados)
# remove pontuações
textos_tratados = gsub("[[:punct:]]", "", textos_tratados)
# remove links
textos_tratados = gsub("http\\w+", "", textos_tratados)
## Visualizar os textos limpos
textos_tratados[1:10]
## Função para transformar todo o texto em letra minúscula
tryTolower = function(x)
{
# cria um dado faltante
y = NA
# faz o tratamento do erro
try_error = tryCatch(tolower(x), error=function(e) e)
# se não der erro, transforma em minúsculo
if (!inherits(try_error, "error"))
y = tolower(x)
# retorna o resultado
return(y)
}
## Usa a função que criamos
textos_tratados = sapply(textos_tratados, tryTolower)
## Remove o nome que foi criado para cada elemento da lista de textos
names(textos_tratados) = NULL
## Remove textos vazios, se existir algum
textos_tratados = textos_tratados[textos_tratados != ""]
## Apresenta o resultado final dos 10 primeiros textos
textos_tratados[1:10]
## Criar uma matriz dos termos e remover as stopwords
termo_por_documento = as.matrix(TermDocumentMatrix(Corpus(VectorSource(textos_tratados)),
control = list(stopwords = c(stopwords("portuguese")))))
## Apresentar apenas os primeiros 10 termos (linhas) com os primeiros 10 documentos (colunas)
termo_por_documento[1:10,1:10]
## Recupera a frequência de cada termo ao somar cada linha e coloca em ordem decrescente
frequencia_dos_termos = sort(rowSums(termo_por_documento), decreasing=TRUE)
## Cria uma tabela com o termo (palavra) e sua respectiva frequência
tabela = data.frame(termo=names(frequencia_dos_termos), frequencia=frequencia_dos_termos)
## Remove o termo mais frequente
tabela = tabela[-1,]
## Desenha a nuvem de palavras
wordcloud(tabela$termo, tabela$frequencia, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
library("twitteR")
library("ROAuth")
consumerKey<-'O9OInB6NRliV3wfWQubw1BHxI'
consumerSecret<-'W0KfrWaOu9rAlmCkaM1He8zByTiPqvcdBeIUMYePy2U6wNkpfb'
accessToken<-'64525214-ksyXxMoPb3HwVGqY1TvQAb61QOa8avz3GKpNUEKV6'
accessTokenSecret<-'Wgz25TPADvinQilQqO0UWF8aOtKCq1trlJyYQ2N1zwcBz'
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
tema <- "masterchef"
quantidade.de.tweets <- 1000
lingua <- "pt"
tweets <- searchTwitter(tema, n=quantidade.de.tweets, lang=lingua)
## Carregar pacotes
library("twitteR")
library("ROAuth")
## Autenticação no twitter
consumerKey<-'O9OInB6NRliV3wfWQubw1BHxI'
consumerSecret<-'W0KfrWaOu9rAlmCkaM1He8zByTiPqvcdBeIUMYePy2U6wNkpfb'
accessToken<-'64525214-ksyXxMoPb3HwVGqY1TvQAb61QOa8avz3GKpNUEKV6'
accessTokenSecret<-'Wgz25TPADvinQilQqO0UWF8aOtKCq1trlJyYQ2N1zwcBz'
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
## Buscar tweets
tema <- "masterchef"
quantidade.de.tweets <- 1000
lingua <- "pt"
tweets <- searchTwitter(tema, n=quantidade.de.tweets, lang=lingua)
## Converter tweets para um dataframe
tweet.df <- twlistToDF(tweets)
View(tweet.df)
## Data-cleaning e criação de uma wordcloud
library(tm)
library(wordcloud)
library(RColorBrewer)
## Recuperar os textos de cada tweet
textos = sapply(tweets, function(x) x$getText())
## Retornar os 10 primeiros tweets para visualizar
textos[1:10]
## Limpar os textos
textos_tratados = textos
# remove entidades de retweets
textos_tratados = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", textos_tratados)
# remove usuários do Twitter
textos_tratados = gsub("@\\w+", "", textos_tratados)
# remove pontuações
textos_tratados = gsub("[[:punct:]]", "", textos_tratados)
# remove links
textos_tratados = gsub("http\\w+", "", textos_tratados)
## Visualizar os textos limpos
textos_tratados[1:10]
## Função para transformar todo o texto em letra minúscula
tryTolower = function(x)
{
# cria um dado faltante
y = NA
# faz o tratamento do erro
try_error = tryCatch(tolower(x), error=function(e) e)
# se não der erro, transforma em minúsculo
if (!inherits(try_error, "error"))
y = tolower(x)
# retorna o resultado
return(y)
}
## Usa a função que criamos
textos_tratados = sapply(textos_tratados, tryTolower)
## Remove o nome que foi criado para cada elemento da lista de textos
names(textos_tratados) = NULL
## Remove textos vazios, se existir algum
textos_tratados = textos_tratados[textos_tratados != ""]
## Apresenta o resultado final dos 10 primeiros textos
textos_tratados[1:10]
## Criar uma matriz dos termos e remover as stopwords
termo_por_documento = as.matrix(TermDocumentMatrix(Corpus(VectorSource(textos_tratados)),
control = list(stopwords = c(stopwords("portuguese")))))
## Apresentar apenas os primeiros 10 termos (linhas) com os primeiros 10 documentos (colunas)
termo_por_documento[1:10,1:10]
## Recupera a frequência de cada termo ao somar cada linha e coloca em ordem decrescente
frequencia_dos_termos = sort(rowSums(termo_por_documento), decreasing=TRUE)
## Cria uma tabela com o termo (palavra) e sua respectiva frequência
tabela = data.frame(termo=names(frequencia_dos_termos), frequencia=frequencia_dos_termos)
## Remove o termo mais frequente
tabela = tabela[-1,]
## Desenha a nuvem de palavras
wordcloud(tabela$termo, tabela$frequencia, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud(tabela$termo, tabela$frequencia, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
warnings()
## Carregar pacotes
library("twitteR")
library("ROAuth")
## Autenticação no twitter
consumerKey<-'O9OInB6NRliV3wfWQubw1BHxI'
consumerSecret<-'W0KfrWaOu9rAlmCkaM1He8zByTiPqvcdBeIUMYePy2U6wNkpfb'
accessToken<-'64525214-ksyXxMoPb3HwVGqY1TvQAb61QOa8avz3GKpNUEKV6'
accessTokenSecret<-'Wgz25TPADvinQilQqO0UWF8aOtKCq1trlJyYQ2N1zwcBz'
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
## Buscar tweets
tema <- "masterchef"
quantidade.de.tweets <- 1000
lingua <- "pt"
tweets <- searchTwitter(tema, n=quantidade.de.tweets, lang=lingua)
## Converter tweets para um dataframe
tweet.df <- twlistToDF(tweets)
View(tweet.df)
## Data-cleaning e criação de uma wordcloud
library(tm)
library(wordcloud)
library(RColorBrewer)
## Recuperar os textos de cada tweet
textos = sapply(tweets, function(x) x$getText())
## Retornar os 10 primeiros tweets para visualizar
textos[1:10]
## Limpar os textos
textos_tratados = textos
# remove entidades de retweets
textos_tratados = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", textos_tratados)
# remove usuários do Twitter
textos_tratados = gsub("@\\w+", "", textos_tratados)
# remove pontuações
textos_tratados = gsub("[[:punct:]]", "", textos_tratados)
# remove links
textos_tratados = gsub("http\\w+", "", textos_tratados)
## Visualizar os textos limpos
textos_tratados[1:10]
## Função para transformar todo o texto em letra minúscula
tryTolower = function(x)
{
# cria um dado faltante
y = NA
# faz o tratamento do erro
try_error = tryCatch(tolower(x), error=function(e) e)
# se não der erro, transforma em minúsculo
if (!inherits(try_error, "error"))
y = tolower(x)
# retorna o resultado
return(y)
}
## Usa a função que criamos
textos_tratados = sapply(textos_tratados, tryTolower)
## Remove o nome que foi criado para cada elemento da lista de textos
names(textos_tratados) = NULL
## Remove textos vazios, se existir algum
textos_tratados = textos_tratados[textos_tratados != ""]
## Apresenta o resultado final dos 10 primeiros textos
textos_tratados[1:10]
## Criar uma matriz dos termos e remover as stopwords
termo_por_documento = as.matrix(TermDocumentMatrix(Corpus(VectorSource(textos_tratados)),
control = list(stopwords = c(stopwords("portuguese")))))
## Apresentar apenas os primeiros 10 termos (linhas) com os primeiros 10 documentos (colunas)
termo_por_documento[1:10,1:10]
## Recupera a frequência de cada termo ao somar cada linha e coloca em ordem decrescente
frequencia_dos_termos = sort(rowSums(m), decreasing=TRUE)
## Cria uma tabela com o termo (palavra) e sua respectiva frequência
tabela = data.frame(termo=names(frequencia_dos_termos), frequencia=frequencia_dos_termos)
## Remove o termo mais frequente
tabela = tabela[-1,]
## Desenha a nuvem de palavras
wordcloud(tabela$termo, tabela$frequencia, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
library("twitteR")
library("ROAuth")
## Autenticação no twitter
consumerKey<-'O9OInB6NRliV3wfWQubw1BHxI'
consumerSecret<-'W0KfrWaOu9rAlmCkaM1He8zByTiPqvcdBeIUMYePy2U6wNkpfb'
accessToken<-'64525214-ksyXxMoPb3HwVGqY1TvQAb61QOa8avz3GKpNUEKV6'
accessTokenSecret<-'Wgz25TPADvinQilQqO0UWF8aOtKCq1trlJyYQ2N1zwcBz'
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
tema <- "masterchef"
quantidade.de.tweets <- 1000
lingua <- "pt"
tweets <- searchTwitter(tema, n=quantidade.de.tweets, lang=lingua)
library(tm)
library(wordcloud)
library(RColorBrewer)
textos = sapply(tweets, function(x) x$getText())
## Retornar os 10 primeiros tweets para visualizar
textos[1:10]
## Limpar os textos
textos_tratados = textos
# remove entidades de retweets
textos_tratados = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", textos_tratados)
# remove usuários do Twitter
textos_tratados = gsub("@\\w+", "", textos_tratados)
# remove pontuações
textos_tratados = gsub("[[:punct:]]", "", textos_tratados)
# remove links
textos_tratados = gsub("http\\w+", "", textos_tratados)
## Visualizar os textos limpos
textos_tratados[1:10]
tryTolower = function(x)
{
# cria um dado faltante
y = NA
# faz o tratamento do erro
try_error = tryCatch(tolower(x), error=function(e) e)
# se não der erro, transforma em minúsculo
if (!inherits(try_error, "error"))
y = tolower(x)
# retorna o resultado
return(y)
}
## Usa a função que criamos
textos_tratados = sapply(textos_tratados, tryTolower)
## Remove o nome que foi criado para cada elemento da lista de textos
names(textos_tratados) = NULL
## Remove textos vazios, se existir algum
textos_tratados = textos_tratados[textos_tratados != ""]
## Apresenta o resultado final dos 10 primeiros textos
textos_tratados[1:10]
## Criar uma matriz dos termos e remover as stopwords
termo_por_documento = as.matrix(TermDocumentMatrix(Corpus(VectorSource(textos_tratados)),
control = list(stopwords = c(stopwords("portuguese")))))
## Apresentar apenas os primeiros 10 termos (linhas) com os primeiros 10 documentos (colunas)
termo_por_documento[1:10,1:10]
## Recupera a frequência de cada termo ao somar cada linha e coloca em ordem decrescente
frequencia_dos_termos = sort(rowSums(termo_por_documento), decreasing=TRUE)
## Cria uma tabela com o termo (palavra) e sua respectiva frequência
tabela = data.frame(termo=names(frequencia_dos_termos), frequencia=frequencia_dos_termos)
## Remove o termo mais frequente
tabela = tabela[-1,]
wordcloud(tabela$termo, tabela$frequencia, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
tweet.df <- twlistToDF(tweets)
View(tweet.df)
tweet.df <- twListToDF(tweets)
View(tweet.df)
## Carregar pacotes
library("twitteR")
library("ROAuth")
## Autenticação no twitter
consumerKey<-'O9OInB6NRliV3wfWQubw1BHxI'
consumerSecret<-'W0KfrWaOu9rAlmCkaM1He8zByTiPqvcdBeIUMYePy2U6wNkpfb'
accessToken<-'64525214-ksyXxMoPb3HwVGqY1TvQAb61QOa8avz3GKpNUEKV6'
accessTokenSecret<-'Wgz25TPADvinQilQqO0UWF8aOtKCq1trlJyYQ2N1zwcBz'
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
## Buscar tweets
tema <- "game of thrones"
quantidade.de.tweets <- 1000
lingua <- "pt"
tweets <- searchTwitter(tema, n=quantidade.de.tweets, lang=lingua)
## Converter tweets para um dataframe
tweet.df <- twListToDF(tweets)
View(tweet.df)
## Data-cleaning e criação de uma wordcloud
library(tm)
library(wordcloud)
library(RColorBrewer)
## Recuperar os textos de cada tweet
textos = sapply(tweets, function(x) x$getText())
## Retornar os 10 primeiros tweets para visualizar
textos[1:10]
## Limpar os textos
textos_tratados = textos
# remove entidades de retweets
textos_tratados = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", textos_tratados)
# remove usuários do Twitter
textos_tratados = gsub("@\\w+", "", textos_tratados)
# remove pontuações
textos_tratados = gsub("[[:punct:]]", "", textos_tratados)
# remove links
textos_tratados = gsub("http\\w+", "", textos_tratados)
## Visualizar os textos limpos
textos_tratados[1:10]
## Função para transformar todo o texto em letra minúscula
tryTolower = function(x)
{
# cria um dado faltante
y = NA
# faz o tratamento do erro
try_error = tryCatch(tolower(x), error=function(e) e)
# se não der erro, transforma em minúsculo
if (!inherits(try_error, "error"))
y = tolower(x)
# retorna o resultado
return(y)
}
## Usa a função que criamos
textos_tratados = sapply(textos_tratados, tryTolower)
## Remove o nome que foi criado para cada elemento da lista de textos
names(textos_tratados) = NULL
## Remove textos vazios, se existir algum
textos_tratados = textos_tratados[textos_tratados != ""]
## Apresenta o resultado final dos 10 primeiros textos
textos_tratados[1:10]
## Criar uma matriz dos termos e remover as stopwords
termo_por_documento = as.matrix(TermDocumentMatrix(Corpus(VectorSource(textos_tratados)),
control = list(stopwords = c(stopwords("portuguese")))))
## Apresentar apenas os primeiros 10 termos (linhas) com os primeiros 10 documentos (colunas)
termo_por_documento[1:10,1:10]
## Recupera a frequência de cada termo ao somar cada linha e coloca em ordem decrescente
frequencia_dos_termos = sort(rowSums(termo_por_documento), decreasing=TRUE)
## Cria uma tabela com o termo (palavra) e sua respectiva frequência
tabela = data.frame(termo=names(frequencia_dos_termos), frequencia=frequencia_dos_termos)
## Remove o termo mais frequente
tabela = tabela[-1,]
## Desenha a nuvem de palavras
wordcloud(tabela$termo, tabela$frequencia, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
tabela = tabela[-1,]
wordcloud(tabela$termo, tabela$frequencia, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
library(rvest)
install.packages('rvest')
library(rvest)
result1 = read_html('https://www.google.com.br/search?q=google+analytics+o+que+é')
## Web Scrapping de Palavras Chave
# Carregando os pacotes
library(rvest)
# Entrar com a palavra chave (implementar)
#palavra = "vestibular"
# Buscando os resultados no Google
result1 = read_html('https://www.google.com.br/search?q=vestibular')
page1 <- result1%>%html_nodes('div cite')%>%html_text()
result2 = read_html('https://www.google.com.br/search?q=vestibular&start=10')
page2 <- result2%>%html_nodes('div cite')%>%html_text()
result3 = read_html('https://www.google.com.br/search?q=vestibular&start=20')
page3 <- result3%>%html_nodes('div cite')%>%html_text()
result4 = read_html('https://www.google.com.br/search?q=vestibular&start=30')
page4 <- result4%>%html_nodes('div cite')%>%html_text()
result5 = read_html('https://www.google.com.br/search?q=vestibular&start=40')
page5 <- result5%>%html_nodes('div cite')%>%html_text()
# Removendo os anuncios
#page1 = page1[2:11]
#page2 = page2[1:10]
#page3 = page3[1:10]
#page4 = page4[1:10]
#page5 = page5[1:10]
# Unindo em um Data Frame
data = cbind.data.frame(page1,page2,page3,page4,page5)
# Escrevendo em um .csv
write.csv2(data, "C:/Users/João/Desktop/google_analytics_vestibular.csv")
## Web Scrapping de Palavras Chave
# Carregando os pacotes
library(rvest)
# Entrar com a palavra chave (implementar)
#palavra = "vestibular"
# Buscando os resultados no Google
result1 = read_html('https://www.google.com.br/search?q=vestibular')
page1 <- result1%>%html_nodes('div cite')%>%html_text()
result2 = read_html('https://www.google.com.br/search?q=vestibular&start=10')
page2 <- result2%>%html_nodes('div cite')%>%html_text()
result3 = read_html('https://www.google.com.br/search?q=vestibular&start=20')
page3 <- result3%>%html_nodes('div cite')%>%html_text()
result4 = read_html('https://www.google.com.br/search?q=vestibular&start=30')
page4 <- result4%>%html_nodes('div cite')%>%html_text()
result5 = read_html('https://www.google.com.br/search?q=vestibular&start=40')
page5 <- result5%>%html_nodes('div cite')%>%html_text()
# Removendo os anuncios
page1 = page1[2:11]
page2 = page2[1:10]
page3 = page3[1:10]
page4 = page4[1:10]
page5 = page5[1:10]
# Unindo em um Data Frame
data = cbind.data.frame(page1,page2,page3,page4,page5)
# Escrevendo em um .csv
write.csv2(data, "C:/Users/João/Desktop/google_analytics_vestibular.csv")
head('google_analytics_vestibular.csv')
print('google_analytics_vestibular.csv')
read.table("google_analytics_vestibular.csv", header=T, dec=",")
getwd()
read.table(C:/Users/João/Documentsgoogle_analytics_vestibular.csv", header=T, dec=",")
read.table("C:/Users/João/Documentsgoogle_analytics_vestibular.csv", header=T, dec=",")
read.table("C:/Users/João/Documents/google_analytics_vestibular.csv", header=T, dec=",")
read.table("C:/Users/João/Desktop/google_analytics_vestibular.csv", header=T, dec=",")
read.csv("google_analytics_vestibular.csv")
read.csv("C:/Users/João/Desktop/google_analytics_vestibular.csv")
teste -> read.csv("google_analytics_vestibular.csv")
teste <- read.csv("google_analytics_vestibular.csv")
teste<- read.csv("C:/Users/João/Desktop/google_analytics_vestibular.csv")
teste
read(teste)
head(teste)
print(teste)
data(teste)
data('teste')
view(teste)
View(teste)
read.table("C:/Users/João/Desktop/google_analytics_vestibular.csv", header=T, dec=";")
read.table("C:/Users/João/Desktop/google_analytics_vestibular.csv", header=f, dec=";")
read.table("C:/Users/João/Desktop/google_analytics_vestibular.csv", header=F, dec=";")
df <- read.table("C:/Users/João/Desktop/google_analytics_vestibular.csv",
header = TRUE,
sep = ";")
df
View(df)
## Web Scrapping de Palavras Chave
# Carregando os pacotes
library(rvest)
# Entrar com a palavra chave (implementar)
#palavra = "palavra chave"
# Buscando os resultados no Google
result1 = read_html('https://www.google.com.br/search?q=google+analytics+o+que+é')
page1 <- result1%>%html_nodes('div cite')%>%html_text()
result2 = read_html('https://www.google.com.br/search?q=google+analytics+o+que+é&start=10')
page2 <- result2%>%html_nodes('div cite')%>%html_text()
result3 = read_html('https://www.google.com.br/search?q=google+analytics+o+que+é&start=20')
page3 <- result3%>%html_nodes('div cite')%>%html_text()
result4 = read_html('https://www.google.com.br/search?q=google+analytics+o+que+é&start=30')
page4 <- result4%>%html_nodes('div cite')%>%html_text()
result5 = read_html('https://www.google.com.br/search?q=google+analytics+o+que+é&start=40')
page5 <- result5%>%html_nodes('div cite')%>%html_text()
# Removendo os anuncios
page1 = page1[2:11]
page2 = page2[1:10]
page3 = page3[1:10]
page4 = page4[1:10]
page5 = page5[1:10]
# Unindo em um Data Frame
data = cbind.data.frame(page1,page2,page3,page4,page5)
# Escrevendo em um .csv
write.csv2(data, "C:/Users/João/Desktop/GoogleScrapping/google_analytics_o_que_é.csv")
## Web Scrapping de Palavras Chave
# Carregando os pacotes
library(rvest)
# Entrar com a palavra chave (implementar)
#palavra = "palavra chave"
# Buscando os resultados no Google
result1 = read_html('https://www.google.com.br/search?q=google+analytics+o+que+é')
page1 <- result1%>%html_nodes('div cite')%>%html_text()
result2 = read_html('https://www.google.com.br/search?q=google+analytics+o+que+é&start=10')
page2 <- result2%>%html_nodes('div cite')%>%html_text()
result3 = read_html('https://www.google.com.br/search?q=google+analytics+o+que+é&start=20')
page3 <- result3%>%html_nodes('div cite')%>%html_text()
result4 = read_html('https://www.google.com.br/search?q=google+analytics+o+que+é&start=30')
page4 <- result4%>%html_nodes('div cite')%>%html_text()
result5 = read_html('https://www.google.com.br/search?q=google+analytics+o+que+é&start=40')
page5 <- result5%>%html_nodes('div cite')%>%html_text()
# Removendo os anuncios
page1 = page1[2:11]
page2 = page2[1:10]
page3 = page3[1:10]
page4 = page4[1:10]
page5 = page5[1:10]
# Unindo em um Data Frame
data = cbind.data.frame(page1,page2,page3,page4,page5)
# Escrevendo em um .csv
write.csv2(data, "C:/Users/João/Desktop/google_analytics_o_que_é.csv")
df <- read.table("C:/Users/João/Desktop/google_analytics_o_que_é.csv",
header = TRUE,
sep = ";")
View(df)
df
cd Desktop
setwd("C:/Users/João/Desktop/estudo/radiohead")
devtools::install_github('charlie86/spotifyr')
library(sentify)
devtools::install_github('charlie86/spotifyr')
library(sentify)
install.packages(sentify)
install.packages('sentify')
library(sentify)
library (installr)
updateR()
